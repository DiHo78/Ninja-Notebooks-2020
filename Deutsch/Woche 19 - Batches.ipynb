{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Woche 19: Batches\r\n",
                "Manchmal kann es vorkommen, dass ihr Operationen auf sehr großen Tabellen machen müsst. Dann kann es vorkommen, dass aufgrund des Transaktions-Verhaltens eurer Datenbank kein Platz mehr in der TempDB vorhanden ist. Das liegt daran, dass Relationale Datenbanken den ACID-Prinzipien folgen, nach denen Operationen Atomar sein sollen, also wird beispielsweise ein `DELETE`-Statement komplett ausgeführt oder zurückgerollt, Zwischenstände, bei denen nur manche Datensätze gelöscht wurden, dürfen nicht vorkommen.  \r\n",
                "## Wie könnt ihr mit Batches arbeiten?\r\n",
                "Um mit Batches zu arbeiten, benötigt ihr `WHILE`-Statements, die wir in der vergangenen Woche vorgestellt hatten. Die Grundidee ist dabei: lösche so lange immer die nächsten N Datensätze bis keine mehr übrig sind. \r\n",
                "Suchen wir uns für diese Fragestellung zunächst die größte Tabelle unserer WideWorldImporters-Datenbank. Um diese zu finden, fragen wir die Metadaten der Datenbank ab: \r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6cc777d0-689b-4257-879c-07fbabad1b1b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "SELECT \r\n",
                "     SCHEMA_NAME(A.schema_id) + '.' + A.Name AS [TableName]\r\n",
                "    ,B.rows as [RowCount]\r\n",
                "FROM sys.objects A\r\n",
                "INNER JOIN sys.partitions B \r\n",
                "ON A.object_id = B.object_id\r\n",
                "WHERE A.type = 'U'\r\n",
                "ORDER BY B.[rows] DESC"
            ],
            "metadata": {
                "azdata_cell_guid": "f3a24102-435f-4407-aa10-c0a6566d645a"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Die größte Tabelle mit über 3,5 Millionen Datensätzen ist dabei die Warehouse.ColdRoomTemperatures_Archive-Tabelle. Sehen wir uns diese Tabelle einmal an, dann stellen wir fest, dass sie die Aufzeichnungen aus dem Zeitraum vom 20.12.2015 bis zum 31.05.2016 enthält: "
            ],
            "metadata": {
                "azdata_cell_guid": "8cdb0099-4405-4d17-8813-d929ea5012dc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "SELECT \r\n",
                " MIN(RecordedWhen)\r\n",
                ",MAX(RecordedWhen) \r\n",
                "FROM Warehouse.ColdRoomTemperatures_Archive "
            ],
            "metadata": {
                "azdata_cell_guid": "307a5c99-81b1-45ca-a0f0-d35cd03fb389"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Hiervon möchten wir nun alle Datensätze löschen, die älter als der 01.04.2016 sind. Wenn wir nachzählen, stellen wir fest, dass es sich dabei immerhin um 2,2 Millionen Datensätze handelt:"
            ],
            "metadata": {
                "azdata_cell_guid": "953b70b2-15a6-4c56-9b92-ca65f7bedb5c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "SELECT \r\n",
                "    COUNT(*)\r\n",
                "FROM Warehouse.ColdRoomTemperatures_Archive \r\n",
                "WHERE RecordedWhen < '2016-04-01'"
            ],
            "metadata": {
                "azdata_cell_guid": "9533bae4-15e0-44ab-9917-333f95f9841d"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Da es sich bei der Tabelle allerdings um eine temporale Tabelle (System versioned table) handelt, können wir aus dieser gar nicht so direkt löschen, kopieren wir sie uns also in ein temporäres Objekt, in dem wir nach Lust und Laune löschen können: "
            ],
            "metadata": {
                "azdata_cell_guid": "65da05c7-6394-4b4c-98d6-6518368adf72"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "DROP TABLE IF EXISTS #WarehouseDeleteTest\r\n",
                "SELECT * INTO #WarehouseDeleteTest FROM Warehouse.ColdRoomTemperatures_Archive"
            ],
            "metadata": {
                "azdata_cell_guid": "8c0443f1-2dbd-49c8-a256-c3c016edc3bd"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Um diese nun für die TempDB und das Transaktionslog ressourcenschonend zu löschen, können wir nun das oben beschriebene Pattern anwenden. Dafür legen wir zunächst eine Variable an, in der wir speichern wie viele Datensätze wir im letzten Schritt gelöscht haben. Dann löschen wir so lange immer 100.000 Datensätze aus der Tabelle bis diese Anzahl null ist, also keine Datensätze zum Löschen mehr übrig sind. Das tun wir jeweils in einer Transaktion, um das Transaktionslog nicht zu sehr zu belasten: "
            ],
            "metadata": {
                "azdata_cell_guid": "c08b4248-a933-47d8-a6fa-4e965675ddef"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "DECLARE @deletedRows int = 1\r\n",
                "\r\n",
                "WHILE @deletedRows > 0\r\n",
                "BEGIN\r\n",
                "    BEGIN TRANSACTION\r\n",
                "    DELETE TOP(100000) \r\n",
                "        #WarehouseDeleteTest\r\n",
                "    WHERE RecordedWhen < '2016-04-01'\r\n",
                "    \r\n",
                "    SET @deletedRows = @@ROWCOUNT\r\n",
                "    COMMIT TRANSACTION\r\n",
                "END"
            ],
            "metadata": {
                "azdata_cell_guid": "fb672f15-8007-4dcf-9859-b3c06eb60b7e"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Wir sehen im Output mehrere Zeilen, die uns mitteilen, dass 100.000 Datensätze gelöscht wurden (`100000 rows affected`). Dann am Ende eine Zeile, bei der weniger als 100.000 Datensätze betroffen sind (`96324 rows affected`) und ganz zum Schluss eine Zeile wo keine Datensätze zum Löschen mehr übriggeblieben sind (`0 rows affected`). \r\n",
                "Zählen wir nun nach, so haben wir in unserer temporären Tabelle nur noch 1,3 Millionen Datensätze übrig: "
            ],
            "metadata": {
                "azdata_cell_guid": "a9bc2d41-35b0-4ad1-aa90-bdbd36e4aa52"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "select count(*) \r\n",
                "FROM #WarehouseDeleteTest"
            ],
            "metadata": {
                "azdata_cell_guid": "12784327-e7d0-4c4c-9773-b3b75bab4214"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Und wenn wir zu guter Letzt noch prüfen, was das Maximale und Minimale Datum dieser Datensätze ist, so stellen wir fest, dass wie erwartet alle Datensätze vor dem ersten April 2016 gelöscht wurden: "
            ],
            "metadata": {
                "azdata_cell_guid": "71f288de-1953-4cfd-b760-4a709c5dc59b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "SELECT \r\n",
                " MIN(RecordedWhen)\r\n",
                ",MAX(RecordedWhen)\r\n",
                "FROM #WarehouseDeleteTest"
            ],
            "metadata": {
                "azdata_cell_guid": "d1ff5678-fcb9-4edd-983a-8998a3c7d478"
            },
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": [
                "Dieses Vorgehen ist immer dann sinnvoll, wenn ihr große Datenmengen verarbeiten möchtet, euch die transaktionale Sicherheit aber nicht so wichtig ist, dass das in einem atomaren Schritt erfolgen muss. Beispielsweise lässt es sich im Data Warehousing-Bereich gut anwenden, wenn aus einer Staging-Tabelle in eine DWH-Tabelle gemerged werden soll ( `MERGE` Statements hatten wir in Woche 15 beschrieben) und die Quell-Tabelle so viele Datensätze enthält, dass das `MERGE` Statement sämtliche Hardware-Ressourcen sprengt. Wichtig ist nur, dass ihr euch bewusst seid, dass ihr hier die ACID-Prinzipien außer Gefecht setzt und euch um entsprechende Fehlerbehandlung und Wiederaufsetzbarkeit kümmert. \n",
                "## Referenzen\n",
                "- [Blogbeitrag über Batched Deletes](https://sqlperformance.com/2013/03/io-subsystem/chunk-deletes)"
            ],
            "metadata": {
                "azdata_cell_guid": "b7ff4c07-c6d5-420d-944f-2970ab4f1317"
            }
        }
    ]
}